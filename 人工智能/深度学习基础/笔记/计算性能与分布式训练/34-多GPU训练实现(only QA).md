## 34 多GPU训练实现
[multiple-gpus](../../代码/computational-performance/multiple-gpus.md)
[multiple-gpus-concise](../../代码/computational-performance/multiple-gpus-concise.md)
# 多GPU训练
:label:`sec_multi_gpu`
到目前为止，我们讨论了如何在CPU和GPU上高效地训练模型，同时在 :numref:`sec_auto_para`中展示了深度学习框架如何在CPU和GPU之间自动地并行化计算和通信，还在 :numref:`sec_use_gpu`中展示了如何使用`nvidia-smi`命令列出计算机上所有可用的GPU。
但是我们没有讨论如何真正实现深度学习训练的并行化。
是否一种方法，以某种方式分割数据到多个设备上，并使其能够正常工作呢？
本节将详细介绍如何从零开始并行地训练网络，这里需要运用小批量随机梯度下降算法（详见 :numref:`sec_minibatch_sgd`）。
后面我还讲介绍如何使用高级API并行训练网络（请参阅 :numref:`sec_multi_gpu_concise`）。
## 问题拆分
我们从一个简单的计算机视觉问题和一个稍稍过时的网络开始。
这个网络有多个卷积层和汇聚层，最后可能有几个全连接的层，看起来非常类似于LeNet :cite:`LeCun.Bottou.Bengio.ea.1998`或AlexNet :cite:`Krizhevsky.Sutskever.Hinton.2012`。
假设我们有多个GPU（如果是桌面服务器则有$2$个，AWS g4dn.12xlarge上有$4$个，p3.16xlarge上有$8$个，p2.16xlarge上有$16$个）。
我们希望以一种方式对训练进行拆分，为实现良好的加速比，还能同时受益于简单且可重复的设计选择。
毕竟，多个GPU同时增加了内存和计算能力。
简而言之，对于需要分类的小批量训练数据，我们有以下选择。
第一种方法，在多个GPU之间拆分网络。
也就是说，每个GPU将流入特定层的数据作为输入，跨多个后续层对数据进行处理，然后将数据发送到下一个GPU。
与单个GPU所能处理的数据相比，我们可以用更大的网络处理数据。
此外，每个GPU占用的*显存*（memory footprint）可以得到很好的控制，虽然它只是整个网络显存的一小部分。
然而，GPU的接口之间需要的密集同步可能是很难办的，特别是层之间计算的工作负载不能正确匹配的时候，还有层之间的接口需要大量的数据传输的时候（例如：激活值和梯度，数据量可能会超出GPU总线的带宽）。
此外，计算密集型操作的顺序对拆分来说也是非常重要的，这方面的最好研究可参见 :cite:`Mirhoseini.Pham.Le.ea.2017`，其本质仍然是一个困难的问题，目前还不清楚研究是否能在特定问题上实现良好的线性缩放。
综上所述，除非存框架或操作系统本身支持将多个GPU连接在一起，否则不建议这种方法。
第二种方法，拆分层内的工作。
例如，将问题分散到$4$个GPU，每个GPU生成$16$个通道的数据，而不是在单个GPU上计算$64$个通道。
对于全连接的层，同样可以拆分输出单元的数量。
 :numref:`fig_alexnet_original`描述了这种设计，其策略用于处理显存非常小（当时为2GB）的GPU。
当通道或单元的数量不太小时，使计算性能有良好的提升。
此外，由于可用的显存呈线性扩展，多个GPU能够处理不断变大的网络。
![由于GPU显存有限，原有AlexNet设计中的模型并行](../imgs/34-多gpu训练/alexnet-original.svg)
:label:`fig_alexnet_original`
然而，我们需要大量的同步或*屏障操作*（barrier operation），因为每一层都依赖于所有其他层的结果。
此外，需要传输的数据量也可能比跨GPU拆分层时还要大。
因此，基于带宽的成本和复杂性，我们同样不推荐这种方法。
最后一种方法，跨多个GPU对数据进行拆分。
这种方式下，所有GPU尽管有不同的观测结果，但是执行着相同类型的工作。
在完成每个小批量数据的训练之后，梯度在GPU上聚合。
这种方法最简单，并可以应用于任何情况，同步只需要在每个小批量数据处理之后进行。
也就是说，当其他梯度参数仍在计算时，完成计算的梯度参数就可以开始交换。
而且，GPU的数量越多，小批量包含的数据量就越大，从而就能提高训练效率。
但是，添加更多的GPU并不能让我们训练更大的模型。
![在多个GPU上并行化。从左到右：原始问题、网络并行、分层并行、数据并行](../imgs/34-多gpu训练/splitting.svg)
:label:`fig_splitting`
 :numref:`fig_splitting`中比较了多个GPU上不同的并行方式。
总体而言，只要GPU的显存足够大，数据并行是最方便的。
有关分布式训练分区的详细描述，请参见 :cite:`Li.Andersen.Park.ea.2014`。
在深度学习的早期，GPU的显存曾经是一个棘手的问题，然而如今除了非常特殊的情况，这个问题已经解决。
下面我们将重点讨论数据并行性。
## 数据并行性
假设一台机器有$k$个GPU。
给定需要训练的模型，虽然每个GPU上的参数值都是相同且同步的，但是每个GPU都将独立地维护一组完整的模型参数。
例如， :numref:`fig_data_parallel`演示了在$k=2$时基于数据并行方法训练模型。
![利用两个GPU上的数据，并行计算小批量随机梯度下降](../imgs/34-多gpu训练/data-parallel.svg)
:label:`fig_data_parallel`
一般来说，$k$个GPU并行训练过程如下：
* 在任何一次训练迭代中，给定的随机的小批量样本都将被分成$k$个部分，并均匀地分配到GPU上；
* 每个GPU根据分配给它的小批量子集，计算模型参数的损失和梯度；
* 将$k$个GPU中的局部梯度聚合，以获得当前小批量的随机梯度；
* 聚合梯度被重新分发到每个GPU中；
* 每个GPU使用这个小批量随机梯度，来更新它所维护的完整的模型参数集。
在实践中请注意，当在$k$个GPU上训练时，需要扩大小批量的大小为$k$的倍数，这样每个GPU都有相同的工作量，就像只在单个GPU上训练一样。
因此，在16-GPU服务器上可以显著地增加小批量数据量的大小，同时可能还需要相应地提高学习率。
还请注意， :numref:`sec_batch_norm`中的批量规范化也需要调整，例如，为每个GPU保留单独的批量规范化参数。

Q1: keras从tf分离，书籍会不会需要重新整理？
> 暂时不会有影响
Q2: 是否可以通过把resnet中的卷积层全替换成mlp来实现一个很深的网络？
> 可以，有这样做的paper，但是通过一维卷积（等价于全连接层）做的，如果直接换成全连接层很可能会过拟合。
Q3: 为什么batch norm是一种正则但只加快训练不提升精度？
> 老师也不太清楚并认为这是很好的问题，可以去查阅论文。
Q4: all_reduce, all_gather主要起什么作用？实际使用时发现pytorch的类似分布式op不能传导梯度，会破坏计算图不能自动求导，如何解决？
> all_reduce是把n个东西加在一起再把所有东西复制回去，all_gather则只是把来自不同地方东西合并但不相加。使用分布式的东西会破坏自动求导，跨GPU的自动求导并不好做，老师不确定pytorch能不能做到这一功能，如果不能就只能手写。
Q5: 两个GPU训练时最后的梯度是把两个GPU上的梯度相加吗？
> 是的。mini-batch的梯度就是每个样本的梯度求和，多GPU时同理，每个GPU向将自己算的那部分样本梯度求和，最后再将两个GPU的计算得的梯度求和。
Q6: 为什么参数大的模型不一定慢？flop数多的模型性能更好是什么原理？
> 性能取决于每算一个乘法需要访问多少个bit，计算量与内存访问的比值越高越好。通常CPU/GPU不会被卡在频率上而是访问数据/内存上，所以参数量小，算力高的模型性能较好（如卷积，矩阵乘法）。
Q7: 为什么分布到多GPU上测试精度会比单GPU抖动大？
> 抖动是因为学习率变大了，使用GPU数对测试精度没有影响，只会影响性能。但为了得到更好的速度需要把batchsize调大，使得收敛情况发生变化，把学习率上调就使得精度更抖。
Q8: batchsize太大会导致loss nan吗？
> 不会，batchsize中的loss是求均值的，理论上batchsize更大数值稳定性会更好，出现数值不稳定问题可能是学习率没有调好。
Q9: GPU显存如何优化？
> 显存手动优化很难，靠的是框架，pytorch的优化做的还不错。除非特别懂框架相关技术不然建议把batchsize调小或是把模型做简单一点。
Q10: 对于精度来说batchsize=1是一种最好的情况吗？
> 可能是。
Q11: parameter server可以和pytorch结合吗，具体如何实现？
> pytorch没有实现parameter server，但mxnet和tensorflow有。但是有第三方实现如byteps支持pytorch。
Q12: 用了nn.DataParallel()，是不是数据集也被自动分配到了多个GPU上？
> 是的。在算net.forward()的时候会分开。
Q13: 验证集准确率震荡大那个参数影响最大？
> 学习率。
Q14: 为了让网络前几层能够训练能否采用不同stage采用不同学习率的方法？
> 可以，主要的问题是麻烦，不好确定各部分学习率相差多少。
Q15: 在用torch的数据并行中将inputs和labels放到GPU0是否会导致性能问题，因为这些数据最终回被挪一次到其他GPU上。
> 数据相比梯度来说很少，不会对性能有太大影响。但这个操作看上去的确很多余，老师认为不需要做，但不这样做会报错。
Q16: 为什么batchsize较小精度会不怎么变化？
> 学习率太大了，batchsize小学习率就不能太大。
Q17: 使用两块不同型号GPU影响深度学习性能吗？
> 需要算好两块GPU的性能差。如一块GPU的性能是另一块的2倍，那么在分配任务时也应该分得2倍的任务量。保证各GPU在同样时间内算完同一部分。
Q18: 课内竞赛直接用教材的VGG11但不收敛，同样的dataloader用resnet可以收敛，如何解决这一问题？
> 可能是学习率太大，也可考虑加入batch normalization。
