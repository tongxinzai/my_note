[深度学习模型压缩与加速综述](C:\Users\hqq\Desktop\文献模型压缩\中文综述\_深度学习模型压缩与加速综述.pdf)

|压缩参数|参数剪枝| 设计关于参数重要性的评价准则,基于该准则判断网络参数的重要程度,删除冗余参数|
|---|---|---|
||参数量化| 将网络参数从32 位全精度浮点数量化到更低位数|
||低秩分解 |将高维参数向量降维分解为稀疏的低维向量|
|压缩结构|参数共享| 利用结构化矩阵或聚类等方法映射网络内部参数|
||知识蒸馏| 将较大的教师模型的信息提炼到较小的学生模型|
|混合方式|混合方式 |参数剪枝、参数量化、低秩分解和参数共享经常组合使用|

1. 参数剪枝是指在预训练好的大型模型的基础上,设计对网络参数的评价准则,以此为根据删除“冗余”参数.根据剪枝粒度粗细,参数剪枝可分为非结构化剪枝和结构化剪枝.非结构化剪枝的粒度比较细,可以无限制地去掉网络中期望比例的任何“冗余”参数,但这样会带来裁剪后网络结构不规整、难以有效加速的问题.结构化剪枝的粒度比较粗,剪枝的最小单位是filter 内参数的组合,通过对filter 或者feature map 设置评价因子,甚至可以删除整个filter 或者某几个channel,使网络“变窄”,从而可以直接在现有软/硬件上获得有效加速,但可能会带来预测精度(accuracy)的下降,需要通过对模型微调(fine-tuning)以恢复性能。
2. 参数量化是指用较低位宽表示典型的32位浮点网络参数,网络参数包括权重、激活值、梯度和误差等等,可以使用统一的位宽(如16-bit、8-bit、2-bit和1-bit等),也可以根据经验或一定策略自由组合不同的位宽.参数量化的优点是:(1)能够显著减少参数存储空间与内存占用空间,将参数从32位浮点型量化到8位整型,从而缩小75%的存储空间,这对于计算资源有限的边缘设备和嵌入式设备进行深度学习模型的部署和使用都有很大的帮助;(2)能够加快运算速度,降低设备能耗,读取32位浮点数所需的带宽可以同时读入4个8位整数,并且整型运算相比浮点型运算更快,自然能够降低设备功耗.但其仍存在一定的局限性,网络参数的位宽减少损失了一部分信息量,会造成推理精度的下降,虽然能够通过微调恢复部分精确度,但也带来时间成本的增加;量化到特殊位宽时,很多现有的训练方法和硬件平台不再适用,需要设计专用的系统架构,灵活性不高.
3. 神经网络的filter可以看作是四维张量:宽度w×高度h×通道数c×卷积核数n,由于c和n对网络结构的整体影响较大,所以基于卷积核(w×h)矩阵信息冗余的特点及其低秩特性,可以利用低秩分解方法进行网络压缩.低秩分解是指通过合并维数和施加低秩约束的方式稀疏化卷积核矩阵,由于权值向量大多分布在低秩子空间,所以可以用少数的基向量来重构卷积核矩阵,达到缩小存储空间的目的.低秩分解方法在大卷积核和中小型网络上有不错的压缩和加速效果,过去的研究已经比较成熟,但近两年已不再流行.原因在于:除了矩阵分解操作成本高、逐层分解不利于全局参数压缩,需要大量的重新训练才能达到收敛等问题之外,近两年提出的新网络越来越多地采用1×1卷积,这种小卷积核不利于低秩分解方法的使用,很难实现网络压缩与加速
4. 参数共享是指利用结构化矩阵或聚类等方法映射网络参数,减少参数数量.参数共享方法的原理与参数剪枝类似,都是利用参数存在大量冗余的特点,目的都是为了减少参数数量.但与参数剪枝直接裁剪不重要的参数不同,参数共享设计一种映射形式,将全部参数映射到少量数据上,减少对存储空间的需求.由于全连接层参数数量较多,参数存储占据整个网络模型的大部分,所以参数共享对于去除全连接层冗余性能够发挥较好的效果;也由于其操作简便,适合与其他方法组合使用.但其缺点在于不易泛化,如何应用于去除卷积层的冗余性仍是一个挑战.同时,对于结构化矩阵这一常用映射形式,很难为权值矩阵找到合适的结构化矩阵,并且其理论依据不够充足.
5. 以上4种利用参数冗余性减少参数数量或者降低参数精度的方法虽然能够精简网络结构,但往往需要庞大的预训练模型,在此基础上进行参数压缩,并且这些方法大都存在精确度下降的问题,需要微调来提升网络性能.设计更紧凑的新型网络结构,是一种新兴的网络压缩与加速理念,构造特殊结构的filter、网络层甚至网络,从头训练,获得适宜部署到移动平台等资源有限设备的网络性能,不再需要像参数压缩类方法那样专门存储预训练模型,也不需要通过微调来提升性能,降低了时间成本,具有存储量小、计算量低和网络性能好的特点.但其缺点在于:由于其特殊结构很难与其他的压缩与加速方法组合使用,并且泛化性较差,不适合作为预训练模型帮助其他模型训练.
6. 神经架构搜索（Neural Architecture Search，NAS）是一种自动化搜索神经网络架构的方法。它通过在给定的搜索空间中探索和评估不同的网络结构，以找到最优的网络架构，从而提高模型的性能。NAS方法的优点是能够自动化地搜索和优化神经网络结构，提高了模型的性能和泛化能力。然而，NAS方法也面临着计算资源消耗大、搜索空间的定义和搜索策略的选择等挑战。近年来，有许多基于NAS的方法被提出，并在不同的任务和领域取得了显著的成果。
7. 知识蒸馏最早由Buciluǎ等人[146]提出,用以训练带有伪数据标记的强分类器的压缩模型和复制原始分类器的输出.与其他压缩与加速方法只使用需要被压缩的目标网络不同,知识蒸馏法需要两种类型的网络:教师模型和学生模型.预先训练好的教师模型通常是一个大型的神经网络模型,具有很好的性能.如图6所示,将教师模型的softmax层输出作为softtarget与学生模型的softmax层输出作为hardtarget一同送入totalloss计算,指导学生模型训练,将教师模型的知识迁移到学生模型中,使学生模型达到与教师模型相当的性能.学生模型更加紧凑高效,起到模型压缩的目的.知识蒸馏法可使深层网络变浅,极大地降低了计算成本,但也存在其局限性.由于使用softmax层输出作为知识,所以一般多用于具有softmax损失函数的分类任务,在其他任务的泛化性不好;并且就目前来看,其压缩比与蒸馏后的模型性能还存在较大的进步空间.
8.  混合方式：以上这些压缩与加速方法单独使用时能够获得很好的效果,但也都存在各自的局限性,组合使用可使它们互为补充.研究人员通过组合使用不同的压缩与加速方法或者针对不同网络层选取不同的压缩与加速方法,设计了一体化的压缩与加速框架,能够获得更好的压缩比与加速效果.参数剪枝、参数量化、低秩分解和参数共享经常组合使用,极大地降低了模型的内存需求和存储需求,方便模型部署到计算资源有限的移动平台[164].知识蒸馏可以与紧凑网络组合使用,为学生模型选择紧凑的网络结构,在保证压缩比的同时,可提升学生模型的性能.混合方式能够综合各类压缩与加速方法的优势,进一步加强了压缩与加速效果,将会是未来在深度学习模型压缩与加速领域的重要研究方向.


