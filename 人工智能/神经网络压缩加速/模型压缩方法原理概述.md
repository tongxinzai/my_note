**模型压缩提出了三部分优化：**
	1）是减少内存密集的访问量；
	2）提高获取模型参数的时间；
	3）加速模型推理时间。
### 量化
#### 量化概述
- 量化训练 (Quant Aware Training, QAT)
量化训练让模型感知量化运算对模型精度带来的影响，通过 finetune 训练降低量化误差。
- 动态离线量化 (Post Training Quantization Dynamic, PTQ Dynamic)
动态离线量化仅将模型中特定算子的权重从FP32类型映射成 INT8/16 类型。
- 静态离线量化 (Post Training Quantization Static, PTQ Static)
静态离线量化使用少量无标签校准数据，采用 KL 散度等方法计算量化比例因子。

| 量化方法 | 功能 | 经典适用场景 | 使用条件 | 易用性 | 精度损失 | 预期收益 |
| --- | --- | --- | --- | --- | --- | --- |
| 量化训练 (QAT) | 通过Finetune训练将模型量化误差降到最小 | 对量化敏感的场景、模型，例如目标检测、分割、OCR等 | 有大量带标签数据 | 好 | 极小 | 减少存储空间4倍，降低计算内存 |
| 静态离线量化 (PTQ Static) | 通过少量校准数据得到量化模型 | 对量化不敏感的场景，例如图像分类任务 | 有少量无标签数据 | 较好 | 较少 | 减少存储空间4倍，降低计算内存 |
| 动态离线量化 (PTQ Dynamic) | 仅量化模型的可学习权重 | 模型体积大、访存开销大的模型，例如BERT模型 | 无 | 一般 | 一般 | 减少存储空间2/4倍，降低计算内存 |

- 要弄懂模型量化的原理就是要弄懂这种数据映射关系，浮点与定点数据的转换公式如下：
<div style="text-align: center;">
    R = (Q - Z) * S<br>
	Q = R/S + Z
</div>
	- R 表示输入的浮点数据
	- Q 表示量化之后的定点数据
	- Z 表示零点（Zero Point）的数值
	- S 表示缩放因子（Scale）的数值

**量化原理**
- 量化算法原始浮点精度数据与量化后 INT8 数据的转换如下：
<div style="text-align: center;">
    f loat = scale × (uint + offset)
</div>
- 确定后通过原始float32高精度数据计算得到uint8数据的转换即为如下公式所示：
<div style="text-align: center;">
    uint8 = round( f loat/scale) − offset)
</div>
- 若待量化数据的取值范围为 [𝑋𝑚𝑖𝑛, 𝑋𝑚𝑎𝑥]，则 𝑠𝑐𝑎𝑙𝑒 的计算公式如下：
<div style="text-align: center;">
	scale = (xmax − xmin)/(Qmax − Qmin)
</div>
- offset的计算方式如下：
<div style="text-align: center;">
    offset = Qmin − round (xmin /scale)
</div>
#### 感知量化训练
- 感知量化训练（Aware Quantization Training）模型中插入伪量化节点fake quant来模拟量化引入的误差。端测推理的时候折叠fake quant节点中的属性到tensor中，在端测推理的过程中直接使用tensor中带有的量化属性参数
- 伪量化节点
	1. 找到输入数据的分布，即找到 min 和 max 值；
	2. 模拟量化到低比特操作的时候的精度损失，把该损失作用到网络模型中，传递给损失函数，让优化器去在训练过程中对该损失值进行优化。
- 伪量化节点：正向传播 Forward
	- 为了求得网络模型tensor数据精确的Min和Max值，因此在模型训练的时候插入伪量化节点来模拟引入的误差，得到数据的分布。对于每一个算子，量化参数通过下面的方式得到：
<div style="text-align: center;">
    clamp (x, x<sub>min</sub>, x<sub>max</sub>) := min (max (x, x<sub>min</sub>), x<sub>max</sub>)<br>
	Q = R<br>
	S + Z<br>
	S = R<sub>max</sub> − R<sub>min</sub><br>
	Q<sub>max</sub> − Q<sub>min</sub><br>
	Z = Q<sub>max</sub> − R<sub>max</sub> ÷ S<br>
</div>
- 伪量化节点：反向传播 Backward
	- 按照正向传播的公式，如果方向传播的时候对其求导数会导致权重为0，因此反向传播的时候相当于一个直接估算器：
	<div style="text-align: center;">
	    <p>δ<sub>out</sub> = δ<sub>in</sub>, I(x∈S) ∈ S : x : x<sub>min</sub> ≤ x ≤ x<sub>max</sub></p>
	</div>
	- 最终反向传播的时候fake quant节点对数据进行了截断式处理

**伪量化节点：更新Min和Max**
- FakeQuant伪量化节点主要是根据找到的min和max值进行伪量化操作，更新min和max分别为running和moving，跟batch normal中更新 beta 和gamma 算子相同

#### 训练后量化
**动态离线量化（Post Training Quantization Dynamic, PTQ Dynamic）**
- 仅将模型中特定算子的权重从FP32类型映射成 INT8/16 类型
- 主要可以减小模型大小，对特定加载权重费时的模型可以起到一定加速效果
- 但是对于不同输入值，其缩放因子是动态计算，因此动态量化是几种量化方法中性能最差的
- 权重量化成 INT16 类型，模型精度不受影响，模型大小为原始的 1/2。
- 权重量化成 INT8 类型，模型精度会受到影响，模型大小为原始的 1/4

**静态离线量化（Post Training Quantization Static, PTQ Static）**
- 同时也称为校正量化或者数据集量化。使用少量无标签校准数据，核心是计算量化比例因子。使用静态量化后的模型进行预测，在此过程中量化模型的缩放因子会根据输入数据的分布进行调整。
<div style="text-align: center;">
    uint8 = round( float/scale) − offset 
</div>
- 静态离线量化的目标是求取量化比例因子，主要通过对称量化、非对称量化方式来求，而找最大值或者阈值的方法又有MinMax、KLD、ADMM、EQ等方法。

**KL散度校准法：原理**
- KL散度校准法也叫相对熵，其中p表示真实分布，q表示非真实分布或p的近似分布：其中p表示真实分布，q表示非真实分布或p的近似分布
<html>
<head>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>
<body>
    <div style="text-align: center;">
        <p>&#x1D55C;(p &#x2225; q) = &#x2211;<sub>x</sub> p(x) log<sub>2</sub> &#x2D9; p(x) - &#x2211;<sub>x</sub> p(x) log<sub>2</sub> &#x2D9; q(x)</p>
    </div>
</body>
</html>
- 相对熵，用来衡量真实分布与非真实分布的差异大小。目的就是改变量化域，实则就是改变真实的分布，并使得修改后得真实分布在量化后与量化前相对熵越小越好。

#### 端侧量化推理部署
- 端侧量化推理的结构方式主要由3种，分别是下图 (a) Fp32输入Fp32输出、(b) Fp32输入int8输出、(c) int8输入int32输出
- INT8卷积示意图，里面混合里三种不同的模式，因为不同的卷积通过不同的方式进行拼接。使用INT8进行inference时，由于数据是实时的，因此数据需要在线量化，量化的流程如图所示。数据量化涉及Quantize，Dequantize和Requantize等3种操作
### 剪枝
#### 剪枝概述
1. 在内存占用相同情况下，大稀疏模型比小密集模型实现了更高的精度。
2. 经过剪枝之后稀疏模型要优于，同体积非稀疏模型。
3. 资源有限的情况下，剪枝是比较有效的模型压缩策略。
4. 优化点还可以往硬件稀疏矩阵储存方向发展
#### 剪枝分类：结构化剪枝、非结构化剪枝
- Unstructured Pruning（非结构化剪枝）
Pros：剪枝算法简单，模型压缩比高
Cons：精度不可控，剪枝后权重矩阵稀疏，没有专用硬件难以实现压缩加速的效果
- Structured Pruning（结构化剪枝）
Pros：大部分算法在 channel 或者 layer 上进行剪枝，保留原始卷积构，不需要专用硬件来实现
Cons：剪枝算法相对复杂

#### 剪枝流程
对模型进行剪枝三种常见做法：
1. 训练一个模型 -> 对模型进行剪枝 -> 对剪枝后模型进行微调
2. 在模型训练过程中进行剪枝 -> 对剪枝后模型进行微调
3. 进行剪枝 -> 从头训练剪枝后模型

**模型剪枝主要单元**
	- 训练 Training：训练过参数化模型，得到最佳网络性能，以此为基准；
	- 剪枝 Pruning：根据算法对模型剪枝，调整网络结构中通道或层数，得到剪枝后的网络结构；
	- 微调 Finetune：在原数据集上进行微调，用于重新弥补因为剪枝后的稀疏模型丢失的精度性能
### 知识蒸馏
#### 概述
**知识蒸馏主要思想**
	- Student Model 学生模型模仿 Teacher Model 教师模型，二者相互竞争，直到学生模型可以与教师模型持平甚至卓越的表现。
**知识蒸馏组成**
	- 知识蒸馏的算法，主要由：
		1）知识 Knowledge、
		2）蒸馏算法 Distillate、
		3）师生架构三个关键部分组成
#### 知识蒸馏的方式
Knowledge 知识的方式
	1. response-based knowledge
	2. feature-based knowledge
	3. relation-based knowledge
	4. Architecture-base knowledg

**Response-Based Knowledge**
- 主要指Teacher Model 教师模型输出层的特征。主要思想是让 Student Model 学生模型直接学习教师模式的预测结果（Knowledge）。
- Response-based knowledge 主要指 Teacher Model 教师模型最后一层 —— 输出层的特征。其主要思想是让学生模型直接模仿教师模式的最终预测
**Feature-Based Knowledge**
- 深度神经网络善于学习到不同层级的表征，因此中间层和输出层的都可以被用作知识来训练学生模型，中间层学习知识的 Feature-Based Knowledge 对于 Response-Based Knowledge是一个很好的补充，其主要思想是将教师和学生的特征激活进行关联起来。
**Feature-Based Knowledge**
- 虽然基于特征的知识迁移为学生模型的学习提供了良好的信息，但如何有效地从教师模型中选择提示层，从学生模型中选择引导层，仍有待进一步研究。由于提示层和引导层的大小存在显著差异，如何正确匹配教师和学生的特征表示也需要探讨。
**Relation-Based Knowledge**
- 基于 Feature-Based Knowledge 和 Response-Based Knowledge 知识都使用了教师模型中特定层中特征的输出。基于关系的知识进一步探索了不同层或数据样本之间的关系。传统的知识转移方法往往涉及到个体知识的提炼。教师的个人软标签 Soft Label 被直接提炼到学生中，实际上经过提炼的知识不仅包含了特征信息，还包含了数据样本之间的相互关系。
#### 知识蒸馏的方法
- 知识蒸馏可以划分为
	1. Offline Distillation ,指知识渊博教师向传授学生知识；
	2. Online Distillation, 指教师和学生共同学习；
	3. Self-Distillation ,指学生自己学习知识。

**Offline Distillation**
- 大多数蒸馏采用 Offline Distillation，蒸馏过程被分为两个阶段：1）蒸馏前教师模型预训练；2）蒸馏算法迁移知识。因此 Offline Distillation 主要侧重于知识迁移部分。
- 通常采用单向知识转移和两阶段训练过程。在步骤1）中需要教师模型参数量比较大，训练时间比较长，这种方式对学生模型的蒸馏比较高效。

**Online Distillation**
- Online Distillation 主要针对参数量大、精度性能好的教师模式不可获得的情况。教师模型和学生模型同时更新，整个知识蒸馏算法是一种有效的端到端可训练方案。
- Cons：现有的 Online Distillation 往往难以获得在线环境下参数量大、精度性能好的教师模型。

**Self-Distillation**
- 教师模型和学生模型使用相同的网络结构，同样采样端到端可训练方案，属于 Online Distillation 的一种特例