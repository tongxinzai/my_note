| |  |  |
| -------- | -------- | -------------------------------------------------------------------------- |
|     压缩参数      |     参数剪枝     |              设计关于参数重要性的评价准则,基于该准则判断网络参数的重要程度,删除冗余参数                                                              |
|          | 参数量化 | 将网络参数从32 位全精度浮点数量化到更低位数                                |
|          | 低秩分解 | 将高维参数向量降维分解为稀疏的低维向量                                     |
| 压缩结构 | 参数共享 | 利用结构化矩阵或聚类等方法映射网络内部参数                                 |
|          | 知识蒸馏 | 将较大的教师模型的信息提炼到较小的学生模型                                 |
我们工作重心放在剪枝、量化和知识蒸馏三个模型压缩方法上。

**模型压缩提出了三部分优化：**
	1）是减少内存密集的访问量；
	2）提高获取模型参数的时间；
	3）加速模型推理时间。
### 量化
#### 量化概述
- 量化训练 (Quant Aware Training, QAT)
量化训练让模型感知量化运算对模型精度带来的影响，通过 finetune 训练降低量化误差。
- 动态离线量化 (Post Training Quantization Dynamic, PTQ Dynamic)
动态离线量化仅将模型中特定算子的权重从FP32类型映射成 INT8/16 类型。
- 静态离线量化 (Post Training Quantization Static, PTQ Static)
静态离线量化使用少量无标签校准数据，采用 KL 散度等方法计算量化比例因子。

| 量化方法 | 功能 | 经典适用场景 | 使用条件 | 易用性 | 精度损失 | 预期收益 |
| --- | --- | --- | --- | --- | --- | --- |
| 量化训练 (QAT) | 通过Finetune训练将模型量化误差降到最小 | 对量化敏感的场景、模型，例如目标检测、分割、OCR等 | 有大量带标签数据 | 好 | 极小 | 减少存储空间4倍，降低计算内存 |
| 静态离线量化 (PTQ Static) | 通过少量校准数据得到量化模型 | 对量化不敏感的场景，例如图像分类任务 | 有少量无标签数据 | 较好 | 较少 | 减少存储空间4倍，降低计算内存 |
| 动态离线量化 (PTQ Dynamic) | 仅量化模型的可学习权重 | 模型体积大、访存开销大的模型，例如BERT模型 | 无 | 一般 | 一般 | 减少存储空间2/4倍，降低计算内存 |

- 要弄懂模型量化的原理就是要弄懂这种数据映射关系，浮点与定点数据的转换公式如下：
<div style="text-align: center;">
    R = (Q - Z) * S<br>
	Q = R/S + Z
</div>
	- R 表示输入的浮点数据
	- Q 表示量化之后的定点数据
	- Z 表示零点（Zero Point）的数值
	- S 表示缩放因子（Scale）的数值

**量化原理**
- 量化算法原始浮点精度数据与量化后 INT8 数据的转换如下：
<div style="text-align: center;">
    f loat = scale × (uint + offset)
</div>
- 确定后通过原始float32高精度数据计算得到uint8数据的转换即为如下公式所示：
<div style="text-align: center;">
    uint8 = round( f loat/scale) − offset)
</div>
- 若待量化数据的取值范围为 [𝑋𝑚𝑖𝑛, 𝑋𝑚𝑎𝑥]，则 𝑠𝑐𝑎𝑙𝑒 的计算公式如下：
<div style="text-align: center;">
	scale = (xmax − xmin)/(Qmax − Qmin)
</div>
- offset的计算方式如下：
<div style="text-align: center;">
    offset = Qmin − round (xmin /scale)
</div>
#### 感知量化训练
- 感知量化训练（Aware Quantization Training）模型中插入伪量化节点fake quant来模拟量化引入的误差。端测推理的时候折叠fake quant节点中的属性到tensor中，在端测推理的过程中直接使用tensor中带有的量化属性参数
- 伪量化节点
	1. 找到输入数据的分布，即找到 min 和 max 值；
	2. 模拟量化到低比特操作的时候的精度损失，把该损失作用到网络模型中，传递给损失函数，让优化器去在训练过程中对该损失值进行优化。
- 伪量化节点：正向传播 Forward
	- 为了求得网络模型tensor数据精确的Min和Max值，因此在模型训练的时候插入伪量化节点来模拟引入的误差，得到数据的分布。对于每一个算子，量化参数通过下面的方式得到：
<div style="text-align: center;">
    clamp (x, x<sub>min</sub>, x<sub>max</sub>) := min (max (x, x<sub>min</sub>), x<sub>max</sub>)<br>
	Q = R<br>
	S + Z<br>
	S = R<sub>max</sub> − R<sub>min</sub><br>
	Q<sub>max</sub> − Q<sub>min</sub><br>
	Z = Q<sub>max</sub> − R<sub>max</sub> ÷ S<br>
</div>
- 伪量化节点：反向传播 Backward
	- 按照正向传播的公式，如果方向传播的时候对其求导数会导致权重为0，因此反向传播的时候相当于一个直接估算器：
	<div style="text-align: center;">
	    <p>δ<sub>out</sub> = δ<sub>in</sub>, I(x∈S) ∈ S : x : x<sub>min</sub> ≤ x ≤ x<sub>max</sub></p>
	</div>
	- 最终反向传播的时候fake quant节点对数据进行了截断式处理

**伪量化节点：更新Min和Max**
- FakeQuant伪量化节点主要是根据找到的min和max值进行伪量化操作，更新min和max分别为running和moving，跟batch normal中更新 beta 和gamma 算子相同

#### 训练后量化
**动态离线量化（Post Training Quantization Dynamic, PTQ Dynamic）**
- 仅将模型中特定算子的权重从FP32类型映射成 INT8/16 类型
- 主要可以减小模型大小，对特定加载权重费时的模型可以起到一定加速效果
- 但是对于不同输入值，其缩放因子是动态计算，因此动态量化是几种量化方法中性能最差的
- 权重量化成 INT16 类型，模型精度不受影响，模型大小为原始的 1/2。
- 权重量化成 INT8 类型，模型精度会受到影响，模型大小为原始的 1/4

**静态离线量化（Post Training Quantization Static, PTQ Static）**
- 同时也称为校正量化或者数据集量化。使用少量无标签校准数据，核心是计算量化比例因子。使用静态量化后的模型进行预测，在此过程中量化模型的缩放因子会根据输入数据的分布进行调整。
<div style="text-align: center;">
    uint8 = round( float/scale) − offset 
</div>
- 静态离线量化的目标是求取量化比例因子，主要通过对称量化、非对称量化方式来求，而找最大值或者阈值的方法又有MinMax、KLD、ADMM、EQ等方法。

**KL散度校准法：原理**
- KL散度校准法也叫相对熵，其中p表示真实分布，q表示非真实分布或p的近似分布：其中p表示真实分布，q表示非真实分布或p的近似分布
<html>
<head>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
</head>
<body>
    <div style="text-align: center;">
        <p>&#x1D55C;(p &#x2225; q) = &#x2211;<sub>x</sub> p(x) log<sub>2</sub> &#x2D9; p(x) - &#x2211;<sub>x</sub> p(x) log<sub>2</sub> &#x2D9; q(x)</p>
    </div>
</body>
</html>
- 相对熵，用来衡量真实分布与非真实分布的差异大小。目的就是改变量化域，实则就是改变真实的分布，并使得修改后得真实分布在量化后与量化前相对熵越小越好。

#### 端侧量化推理部署
- 端侧量化推理的结构方式主要由3种，分别是下图 (a) Fp32输入Fp32输出、(b) Fp32输入int8输出、(c) int8输入int32输出
- INT8卷积示意图，里面混合里三种不同的模式，因为不同的卷积通过不同的方式进行拼接。使用INT8进行inference时，由于数据是实时的，因此数据需要在线量化，量化的流程如图所示。数据量化涉及Quantize，Dequantize和Requantize等3种操作
### 剪枝
#### 剪枝概述
1. 在内存占用相同情况下，大稀疏模型比小密集模型实现了更高的精度。
2. 经过剪枝之后稀疏模型要优于，同体积非稀疏模型。
3. 资源有限的情况下，剪枝是比较有效的模型压缩策略。
4. 优化点还可以往硬件稀疏矩阵储存方向发展
#### 剪枝分类：结构化剪枝、非结构化剪枝
- Unstructured Pruning（非结构化剪枝）
Pros：剪枝算法简单，模型压缩比高
Cons：精度不可控，剪枝后权重矩阵稀疏，没有专用硬件难以实现压缩加速的效果
- Structured Pruning（结构化剪枝）
Pros：大部分算法在 channel 或者 layer 上进行剪枝，保留原始卷积构，不需要专用硬件来实现
Cons：剪枝算法相对复杂

#### 剪枝流程
对模型进行剪枝三种常见做法：
1. 训练一个模型 -> 对模型进行剪枝 -> 对剪枝后模型进行微调
2. 在模型训练过程中进行剪枝 -> 对剪枝后模型进行微调
3. 进行剪枝 -> 从头训练剪枝后模型

**模型剪枝主要单元**
	- 训练 Training：训练过参数化模型，得到最佳网络性能，以此为基准；
	- 剪枝 Pruning：根据算法对模型剪枝，调整网络结构中通道或层数，得到剪枝后的网络结构；
	- 微调 Finetune：在原数据集上进行微调，用于重新弥补因为剪枝后的稀疏模型丢失的精度性能
### 知识蒸馏
#### 概述
**知识蒸馏主要思想**
	- Student Model 学生模型模仿 Teacher Model 教师模型，二者相互竞争，直到学生模型可以与教师模型持平甚至卓越的表现。
**知识蒸馏组成**
	- 知识蒸馏的算法，主要由：
		1）知识 Knowledge、
		2）蒸馏算法 Distillate、
		3）师生架构三个关键部分组成
#### 知识蒸馏的方式
**蒸馏的知识形式：输出特征知识、中间特征知识、关系特征知识和结构特征知识
(输出特征知识称为 logits-based方法；中间特征知识、关系特征知识和结构特征知识统称为hints-)
- 输出特征知识通常指的是教师模型的最后一层特征(logits)，主要包括逻辑单元和软目标的知识.输出特征知识蒸馏的主要思想是促使学生能够学习到教师模型的最终预测，以达到和教师模型一样的预测性能.

- 中间特征知识：如果网络较深的话，单单学习教师的输出特征知识是不够的.复杂教师和简单学生模型在中间的隐含层之间存在着显著的容量差异，这导致它们不同的特征表达能力.教师的中间特征状态知识可以用于解决教师和学生模型在容量之间存在的“代沟”(Gap)问题，其主要思想是从教师中间的网络层中提取特征来充当学生模型中间层输出的提示(Hint).这一过程称之为中间特征的知识蒸馏。

- 关系特征指的是教师模型不同层和不同数据样本之间的关系知识.关系特征知识蒸馏认为学习的本质不是特征输出的结果，而是层与层之间和样本数据之间的关系.它的重点是提供一个恒等的关系映射使得学生模型能够更好的学习教师模型的关系知识.

- 结构特征知识是教师模型的完整知识体系，不仅包括教师的输出特征知识，中间特征知识和关系特征知识，还包括教师模型的区域特征分布等知识.网络性能不仅取决于网络的参数或关系，而且还取决于它的体系结构.结构特征知识蒸馏是以互补的形式利用多种知识来促使学生的预测能包含和教师一样丰富的结构知识.不同工作构成结构化特征知识的成份是不同的，比如结合样本特征、样本间关系和特征空间变换作为结构化的知识，将成对像素的关系和像素间的整体知识作为结构化知识。

#### 知识蒸馏的方法
- 知识蒸馏可以划分为
	1. Offline Distillation ,指知识渊博教师向传授学生知识；
	2. Online Distillation, 指教师和学生共同学习；
	3. Self-Distillation ,指学生自己学习知识。

**Offline Distillation**
- 大多数蒸馏采用 Offline Distillation，蒸馏过程被分为两个阶段：1）蒸馏前教师模型预训练；2）蒸馏算法迁移知识。因此 Offline Distillation 主要侧重于知识迁移部分。
- 通常采用单向知识转移和两阶段训练过程。在步骤1）中需要教师模型参数量比较大，训练时间比较长，这种方式对学生模型的蒸馏比较高效。

**Online Distillation**
- Online Distillation 主要针对参数量大、精度性能好的教师模式不可获得的情况。教师模型和学生模型同时更新，整个知识蒸馏算法是一种有效的端到端可训练方案。
- Cons：现有的 Online Distillation 往往难以获得在线环境下参数量大、精度性能好的教师模型。

**Self-Distillation**
- 教师模型和学生模型使用相同的网络结构，同样采样端到端可训练方案，属于 Online Distillation 的一种特例