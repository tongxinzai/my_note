[深度神经网络结构搜索综述（2021）](C:\Users\hqq\Desktop\文献\模型压缩\中文综述\_深度神经网络结构搜索综述.pdf)
[Neural Architecture Search: Insights from 1000 Papers](C:\Users\hqq\Desktop\文献\模型压缩\英文综述\神经网络结构搜索\2023Neural Architecture Search-Insights from 1000 Papers.pdf)
神经网络结构搜索可以分成 3个维度:搜索空间(search space)、搜索策略(search strategy)和性能 评估策略( performance estimation strategy)。
1. 搜索空间定义了可以表征的神经网络结构,在搜索空间的设计上嵌入了很多先验知识,这样可以一定程度上减小搜索空间的大小,使搜索变得更简单。
2. 搜索策略则是用来探索搜索空间,通常会面临探索—利用权衡(exploration-exploitationtrade-off),一方面需要尽快找到最优解,另一方面又不能过早地收敛到一个局部最优结构。
3. 神经网络结构搜索的目标是从定义的搜索空间内,面向某个未知数据集,找到具有最高的泛化性能的网络结构。性能评估策略在神经网络结构搜索中则用来估计采样到的神经网络结构的泛化性能。
**搜索空间：搜索空间可以分为宏搜索空间、链式结构搜索空间、基于单元的搜索空间、分层搜索空间以及架构编码等
- 宏搜索空间可以被分为两类：首先是被编码在一个层级上的整个架构的搜索空间，其次是只关注宏级别的超参数。宏搜索空间拥有更好的表示能力以及其灵活结构有助于更好的发现新颖的架构。然而宏搜索空间的运行较慢。
- 链结构搜索空间有简单的拓扑结构，链结构搜索在卷积网络以及transformer架构中较热。链结构搜索空间的概念简单，利于设计与实现。但也由于其结构简单，相对来说难以找到真正新颖的构架。
- 基于单元的搜索空间是NAS搜索空间的热门类型，可搜索的单元构成了搜索空间的微结构，而外部骨架是固定的。基于单元的设计极大的减少了搜索空间的复杂度，并且最终的架构有高质量的表现。然而，有许多特定的设计选择和固定的超参数，其对结果的影响尚不清楚。较小的搜索空间难以被发现新颖的高质量架构
- 分层搜索空间包含多种不同层次的模式设计。简单的二层搜索空间通过将宏架构的超参数加在单元或链式搜索空间中。此外还有三层四层的。分层搜索空间的优势：首先有更好的表现，它可以集合宏空间、链结构与单元搜索空间为一体。其次，分层可以减少搜索的复杂度，提高效率。但是分层搜索空间的实现和搜索不易。
- 架构编码是一种用于表示神经网络架构的编码方式。在神经结构搜索（NAS）中，为了在搜索空间中进行架构搜索，需要一种表示架构的方式，以便进行优化和评估。架构编码是将神经网络架构转化为可供计算和优化的向量或字符串的过程。在选择架构编码时，可扩展性（scalability）和通用性（generalizability）是重要特征。最近的研究表明，不同的NAS子程序，如随机采样架构、扰动架构或训练代理模型，可能在不同的编码方案下表现最佳。此外，即使对架构编码方案进行微小改变，也可能对NAS的性能产生显著影响。
**搜索策略大体上分为两类：黑匣子最优化技术和 one-shot技术。
黑匣子最优化技术相对于one-shot技术相比，具有更好的鲁棒性、不可微目标的更简优化、更简单的并行性、与其他参数的联合优化。但是在对黑匣子优化技术做加速时会产生大量的计算消耗。
- 随机搜索和本地搜索是相当有用的baselines。但随机搜索在大的、多样的网络下表现不佳。
- 强化学习是早期的NAS的重要组成部分，不过现在被进化算法和贝叶斯最优化超过了。
- 进化算法灵活、概念简单，但权重优化通常是由标准的基于SGD的方法来完成的。
- 贝叶斯优化算法通过选择合适的架构来最大化需求函数，该算法在NAS上取得了巨大的成功。
- 蒙特卡洛树搜索（MCTS）通过对新决策递归地采样来找到最优决策、运行随机的rollout来获得reward，最后反向传播更新初始权重。
One-shot 技术：One-shot 方法通过超网络或超网络的单一（“One-shot ”）训练来隐式地训练搜索空间中的所有架构。这样的好处是添加候选算子只会线性增加计算消耗，而其子图会指数级增长。
- 基于超网络不可微分方法（Non-Differentiable Supernet-Based Methods）：这个方法家族中的一些方法将超级网络的训练和架构搜索解耦：首先训练一个超级网络，然后运行一个黑盒优化算法来搜索最佳架构。其他方法在训练超级网络的同时，同时运行一个非可微分的搜索算法，如强化学习，来选择子网络。
- 基于超网络微分方法（Differentiable Supernet-Based Methods）：DARTS是其中的经典方法，之后产生了许多基于该方法的工作。DARTS方法基于一个假设，即训练超网络生成的子网路排位与单独训练自网络的排位得到的结果一致。此外，一些工作表明微分NAS技术相对与其他操作选择更倾向于跳跃连接。微分NAS技术训练过程中对内存的需求较大，许多工作力求缓解该问题。
- 超网络（hypernetwork）：是生成其他神经网络的权重的神经网络。SMASH是第一个将超网络应用于NAS 的方法，