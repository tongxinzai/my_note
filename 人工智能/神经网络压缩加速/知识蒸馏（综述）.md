
[知识蒸馏研究综述](C:\Users\hqq\Desktop\文献\模型压缩\中文综述\_知识蒸馏研究综述.pdf)
[深度学习中知识蒸馏研究综述](C:\Users\hqq\Desktop\文献\模型压缩\中文综述\_深度学习中知识蒸馏研究综述.pdf)
**蒸馏的知识形式：输出特征知识、中间特征知识、关系特征知识和结构特征知识
-  输出特征知识通常指的是教师模型的最后一层特征，主要包括逻辑单元和软目标的知识.输出特征知识蒸馏的主要思想是促使学生能够学习到教师模型的最终预测，以达到和教师模型一样的预测性能.
-  中间特征知识：如果网络较深的话，单单学习教师的输出特征知识是不够的.复杂教师和简单学生模型在中间的隐含层之间存在着显著的容量差异，这导致它们不同的特征表达能力.教师的中间特征状态知识可以用于解决教师和学生模型在容量之间存在的“代沟”(Gap)问题，其主要思想是从教师中间的网络层中提取特征来充当学生模型中间层输出的提示(Hint).这一过程称之为中间特征的知识蒸馏。
- 关系特征指的是教师模型不同层和不同数据样本之间的关系知识.关系特征知识蒸馏认为学习的本质不是特征输出的结果，而是层与层之间和样本数据之间的关系.它的重点是提供一个恒等的关系映射使得学生模型能够更好的学习教师模型的关系知识.
- 结构特征知识是教师模型的完整知识体系，不仅包括教师的输出特征知识，中间特征知识和关系特征知识，还包括教师模型的区域特征分布等知识.网络性能不仅取决于网络的参数或关系，而且还取决于它的体系结构.结构特征知识蒸馏是以互补的形式利用多种知识来促使学生的预测能包含和教师一样丰富的结构知识.不同工作构成结构化特征知识的成份是不同的，比如结合样本特征、样本间关系和特征空间变换作为结构化的知识，将成对像素的关系和像素间的整体知识作为结构化知识。

**知识蒸馏的方法：知识合并、多教师学习、教师助理、跨模态蒸馏、相互蒸馏、终身蒸馏以及自蒸馏.    
-  知识合并是将多个教师或多个任务的知识迁移到单个学生模型中，从而使其可以同时处理多个任务.知识合并的重点是学生应该如何将多个教师的知识用于更新单个学生模型参数，并且训练结束的学生模型能处理多个教师模型原先的任务.
-  知识合并和多教师学习都属于“多教师-单学生”的网络训练结构.它们的相同点是，知识合并和多教师学习都是学习多个教师模型的知识，但是它们的目标却是不一样的.知识合并是要促使学生模型能同时处理多个教师模型原先的任务，而多教师学习是提高学生模型在单个任务上的性能.
- 教师助理：教师和学生模型由于容量差异大导致它们存在着“代沟”.“代沟”既可以通过传递教师的特征知识去缓解，也可以使用教师助理(TeacherAssistant)网络去协助学生模型学习.
-  跨模态蒸馏：在许多实际应用中，数据通常以多种模态存在，一些不同模态的数据均是描述同一个事物或事件，我们可以利用同步的模态信息实现跨模态蒸馏
-  相互蒸馏(MutualDistillation)是让一组未经训练的学生模型同时开始学习，并共同解决任务.它是一种在线的知识蒸馏，即教师和学生模型是同时训练并更新的.深度学习网络在学习新任务时，对旧任务的性能就会急剧下降，这个现象被称为灾难性遗忘.这就需要使用终身学习来减轻这种影响，终身学习也称为持续学习或增量学习.目前，有些工作使用知识蒸馏方法来实现终身学习，称之为终身蒸馏(LifelongDistillation).终身蒸馏就是通过知识蒸馏来保持旧任务和适应新任务的性能，其重点是训练新数据时如何保持旧任务的性能来减轻灾难性遗忘.
-  自蒸馏(Self-Distillation)是单个网络被同时用作教师和学生模型，让单个网络模型在自我学习的过程中通过知识蒸馏去提升性能.它也是一种在线的知识蒸馏.


