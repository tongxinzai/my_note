- **Contrastive Representation Distillation.** *Yonglong Tian, Dilip Krishnan, Phillip Isola.* **International Conference on Learning Representations, 2019** [(PDF)](<../NoteTool/papers/Contrastive Representation Distillation.pdf>)  [(arxiv)](https://arxiv.org/abs/1910.10699.abs)[![citation](https://img.shields.io/badge/dynamic/json?label=citation&query=citationCount&url=https%3A%2F%2Fapi.semanticscholar.org%2Fgraph%2Fv1%2Fpaper%2F197498cb2ad787e4f71c05098cee6b10d9d067bd%3Ffields%3DcitationCount)](https://www.semanticscholar.org/paper/197498cb2ad787e4f71c05098cee6b10d9d067bd)
#### ABSTRACT
通常，我们希望将神经网络的表示知识转移到另一个神经网络中。例如，将一个大型网络压缩成一个较小的网络，将知识从一个感官模态转移到另一个模态，或将一组模型组合成一个估计器等。知识蒸馏是解决这些问题的标准方法，它最小化了教师网络和学生网络之间概率输出的KL散度。我们证明了这个目标忽视了教师网络的重要结构知识。这激发了我们提出了一种替代目标，通过训练学生捕捉教师网络对数据表示的更多信息。我们将这个目标定义为对比学习。实验证明，我们得到的新目标在各种知识转移任务中表现优于知识蒸馏和其他前沿的蒸馏方法，包括单模型压缩、集成蒸馏和跨模态转移。